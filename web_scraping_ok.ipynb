{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabordun/web_scraping/blob/master/web_scraping_ok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSINI_S8QRib",
        "colab_type": "text"
      },
      "source": [
        "**Web scraping project to create a COVID-19 heat map**\n",
        "\n",
        "The aim is to count the uniqe mentions of Coronavirus on the world's most popular websites.\n",
        "\n",
        "The list of the most popular websites comes from:\n",
        "http://www.ebizmba.com/articles/news-websites\n",
        "\n",
        "The script **scrapes this site for the URLs** of the mostly visited news websites worldwide.\n",
        "\n",
        "After getting the URLs, **the script creates a parsed object for every individual websites' startsite content**.\n",
        "\n",
        "Then, according to a pre-defined '**keywords**' list, the script counts the number of mentioning of all the keywords and put the numbers in an **output table**.\n",
        "\n",
        "The output table contains the name & the exact URL of the corresponding website, the number of the keywords' total appearances and a proxy for the size of the site (the last in order to get some comparable measure).\n",
        "\n",
        "Finally, in the last section the output table is saved on Google Drive. After that, the saved file is intended to import into Tableau to create a visualization of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivafqztuo-Os",
        "colab_type": "code",
        "outputId": "2e5ab1ed-2f9a-4b19-e195-a033ce80df1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "#get the URLs of the news sites\n",
        "\n",
        "#import packages\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "#import requests\n",
        "\n",
        "from urllib import request\n",
        "import urllib\n",
        "import requests\n",
        "\n",
        "#finding sources\n",
        "\n",
        "\n",
        "url = 'http://www.ebizmba.com/articles/news-websites'\n",
        "\n",
        "def getAllUrl(url):\n",
        "    try:\n",
        "        page = request.urlopen(url).read()\n",
        "    except:\n",
        "        return []\n",
        "    urlList = []\n",
        "    try:\n",
        "        soup = BeautifulSoup(page)\n",
        "        soup.prettify()\n",
        "        for anchor in soup.findAll('a', href=True):\n",
        "            if not 'http://' in anchor['href']:\n",
        "                if urlparse.urljoin(url, anchor['href']) not in urlList:\n",
        "                    urlList.append(urlparse.urljoin(url, anchor['href']))\n",
        "            else:\n",
        "                if anchor['href'] not in urlList:\n",
        "                    urlList.append(anchor['href'])\n",
        "\n",
        "        length = len(urlList)\n",
        "\n",
        "        return urlList\n",
        "    except request.HTTPError as e:\n",
        "        print(e)\n",
        "\n",
        "print(getAllUrl(url))\n",
        "\n",
        "result_url_list= getAllUrl(url)\n",
        "not_needed = ['ebizmba','alexa','quantcast','siteanalytics']\n",
        "\n",
        "#filter unnecessary items\n",
        "\n",
        "for item in getAllUrl(url):\n",
        " for i in not_needed:\n",
        "  if i in (item):\n",
        "   result_url_list.remove(item)\n",
        "\n",
        "print(result_url_list)\n",
        "print(len(getAllUrl(url)))\n",
        "len(result_url_list)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['http://www.ebizmba.com/', 'http://www.ebizmba.com/directory#admin', 'http://www.ebizmba.com/directory#design', 'http://www.ebizmba.com/directory#marketing', 'http://www.ebizmba.com/directory#ecommerce', 'http://www.ebizmba.com/directory', 'http://www.ebizmba.com/directory#media', 'http://www.ebizmba.com/directory#research', \"javascript:bookmarksite('eBizMBA | The eBusiness Knowledgebase', 'http://www.ebizmba.com')\", 'http://www.ebizmba.com/terms', 'http://www.ebizmba.com/privacy', 'http://news.yahoo.com', 'http://siteanalytics.compete.com/news.yahoo.com', 'http://quantcast.com/news.yahoo.com', 'http://www.alexa.com/siteinfo/news.yahoo.com/', 'http://news.google.com', 'http://siteanalytics.compete.com/news.google.com', 'http://quantcast.com/news.google.com', 'http://www.alexa.com/siteinfo/news.google.com/', 'http://www.huffingtonpost.com', 'http://siteanalytics.compete.com/huffingtonpost.com', 'http://quantcast.com/huffingtonpost.com', 'http://www.alexa.com/siteinfo/huffingtonpost.com/', 'http://www.cnn.com', 'http://siteanalytics.compete.com/cnn.com', 'http://quantcast.com/cnn.com', 'http://www.alexa.com/siteinfo/cnn.com/', 'http://www.nytimes.com', 'http://siteanalytics.compete.com/nytimes.com', 'http://quantcast.com/nytimes.com', 'http://www.alexa.com/siteinfo/nytimes.com/', 'http://www.foxnews.com', 'http://siteanalytics.compete.com/foxnews.com', 'http://quantcast.com/foxnews.com', 'http://www.alexa.com/siteinfo/foxnews.com/', 'http://www.nbcnews.com', 'http://siteanalytics.compete.com/nbcnews.com', 'http://quantcast.com/nbcnews.com', 'http://www.alexa.com/siteinfo/nbcnews.com/', 'http://www.dailymail.co.uk', 'http://siteanalytics.compete.com/dailymail.co.uk', 'http://quantcast.com/dailymail.co.uk', 'http://www.alexa.com/siteinfo/dailymail.co.uk', 'http://www.washingtonpost.com', 'http://siteanalytics.compete.com/washingtonpost.com', 'http://quantcast.com/washingtonpost.com', 'http://www.alexa.com/siteinfo/washingtonpost.com/', 'http://www.theguardian.com', 'http://siteanalytics.compete.com/theguardian.com', 'http://quantcast.com/theguardian.com', 'http://www.alexa.com/siteinfo/theguardian.com/', 'http://www.wsj.com', 'http://siteanalytics.compete.com/wsj.com', 'http://quantcast.com/wsj.com', 'http://www.alexa.com/siteinfo/wsj.com/', 'http://www.abcnews.go.com', 'http://siteanalytics.compete.com/abcnews.go.com', 'http://quantcast.com/abcnews.go.com', 'http://www.alexa.com/siteinfo/abcnews.go.com/', 'http://news.bbc.co.uk', 'http://siteanalytics.compete.com/news.bbc.co.uk', 'http://quantcast.com/news.bbc.co.uk', 'http://www.alexa.com/siteinfo/news.bbc.co.uk/', 'http://www.usatoday.com', 'http://siteanalytics.compete.com/usatoday.com', 'http://quantcast.com/usatoday.com', 'http://www.alexa.com/siteinfo/usatoday.com/', 'http://www.latimes.com', 'http://siteanalytics.compete.com/latimes.com', 'http://quantcast.com/latimes.com', 'http://www.alexa.com/siteinfo/latimes.com/', 'http://www.ebizmba.com/articles/best-flash-sites', 'http://www.ebizmba.com/articles/best-html5-websites', 'http://www.ebizmba.com/articles/blogs', 'http://www.ebizmba.com/articles/business-websites', 'http://www.ebizmba.com/articles/car-websites', 'http://www.ebizmba.com/articles/coupon-websites', 'http://www.ebizmba.com/articles/dating-websites', 'http://www.ebizmba.com/articles/design-websites', 'http://www.ebizmba.com/articles/ebusiness-websites', 'http://www.ebizmba.com/articles/file-sharing-websites', 'http://www.ebizmba.com/articles/funny-websites', 'http://www.ebizmba.com/articles/gadget-websites', 'http://www.ebizmba.com/articles/game-websites', 'http://www.ebizmba.com/articles/gossip-websites', 'http://www.ebizmba.com/articles/health-websites', 'http://www.ebizmba.com/articles/job-websites', 'http://www.ebizmba.com/articles/kids-websites', 'http://www.ebizmba.com/articles/media-websites', 'http://www.ebizmba.com/articles/most-popular-websites', 'http://www.ebizmba.com/articles/movie-websites', 'http://www.ebizmba.com/articles/music-websites', 'http://www.ebizmba.com/articles/news-websites', 'http://www.ebizmba.com/articles/people-search', 'http://www.ebizmba.com/articles/personal-finance-websites', 'http://www.ebizmba.com/articles/photo-sharing-sites', 'http://www.ebizmba.com/articles/political-websites', 'http://www.ebizmba.com/articles/real-estate-websites', 'http://www.ebizmba.com/articles/recipe-websites', 'http://www.ebizmba.com/articles/reference-websites', 'http://www.ebizmba.com/articles/science-websites', 'http://www.ebizmba.com/articles/search-engines', 'http://www.ebizmba.com/articles/seo-websites', 'http://www.ebizmba.com/articles/shopping-websites', 'http://www.ebizmba.com/articles/social-bookmarking-websites', 'http://www.ebizmba.com/articles/social-networking-websites', 'http://www.ebizmba.com/articles/sports-websites', 'http://www.ebizmba.com/articles/travel-websites', 'http://www.ebizmba.com/articles/video-game-websites', 'http://www.ebizmba.com/articles/video-websites', 'http://www.ebizmba.com/articles/viral-sites', 'http://www.ebizmba.com/articles/web-2.0-websites', 'http://www.ebizmba.com/articles/web-hosting', 'http://www.ebizmba.com/company', 'http://www.ebizmba.com/contact']\n",
            "['http://news.yahoo.com', 'http://news.google.com', 'http://www.huffingtonpost.com', 'http://www.cnn.com', 'http://www.nytimes.com', 'http://www.foxnews.com', 'http://www.nbcnews.com', 'http://www.dailymail.co.uk', 'http://www.washingtonpost.com', 'http://www.theguardian.com', 'http://www.wsj.com', 'http://www.abcnews.go.com', 'http://news.bbc.co.uk', 'http://www.usatoday.com', 'http://www.latimes.com']\n",
            "115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t60DWdLQpTfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get parsed objects from each URLs - optional\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for url in result_url_list:\n",
        "  try: \n",
        "   basic=requests.get(url).text\n",
        "   soup=BeautifulSoup(basic, 'html.parser')\n",
        "  except:\n",
        "   pass\n",
        "  \n",
        "  #stripped_text=soup.get_text()\n",
        "  corpus.append(soup)\n",
        "\n",
        "len(corpus) \n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svkmk540GGpZ",
        "colab_type": "code",
        "outputId": "a474baff-f86a-4a83-9e7a-f0b1b49665e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# run keyword(s) counter, create output table\n",
        "\n",
        "table = {'name': [], 'number of mentioning':[] , 'site size': [],'url':[] }\n",
        "\n",
        "keywords=['corona','Corona','coronavirus','corona virus','Corona Virus', 'Coronavirus', 'COVID', 'COVID-19','Covid']\n",
        "\n",
        "for url in result_url_list:\n",
        "  try: \n",
        "   basic=requests.get(url).text\n",
        "   soup=BeautifulSoup(basic, 'html.parser')\n",
        "  except:\n",
        "   pass\n",
        "  \n",
        "  counter=0\n",
        "\n",
        "  for i in keywords:\n",
        "   y=len(soup.body.find_all(text=re.compile(i)))\n",
        "   counter += y\n",
        "  \n",
        "  size = len(soup.find_all('a'))\n",
        "\n",
        "  print(url)\n",
        "  print(counter)\n",
        "  print(size)\n",
        "  print('--')\n",
        "\n",
        "  table['name'].append(urllib.parse.urlsplit(url)[1].replace('www.','').replace('.com','').replace('.co.uk',''))\n",
        "  table['number of mentioning'].append(counter)\n",
        "  table['site size'].append(size)\n",
        "  table['url'].append(url)\n",
        "\n",
        "output_df = pd.DataFrame(data = table)\n",
        "print(output_df)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://news.yahoo.com\n",
            "78\n",
            "118\n",
            "--\n",
            "http://news.google.com\n",
            "198\n",
            "761\n",
            "--\n",
            "http://www.huffingtonpost.com\n",
            "0\n",
            "0\n",
            "--\n",
            "http://www.cnn.com\n",
            "2\n",
            "230\n",
            "--\n",
            "http://www.nytimes.com\n",
            "38\n",
            "128\n",
            "--\n",
            "http://www.foxnews.com\n",
            "188\n",
            "809\n",
            "--\n",
            "http://www.nbcnews.com\n",
            "145\n",
            "424\n",
            "--\n",
            "http://www.dailymail.co.uk\n",
            "159\n",
            "864\n",
            "--\n",
            "http://www.washingtonpost.com\n",
            "65\n",
            "562\n",
            "--\n",
            "http://www.theguardian.com\n",
            "139\n",
            "367\n",
            "--\n",
            "http://www.wsj.com\n",
            "9\n",
            "292\n",
            "--\n",
            "http://www.abcnews.go.com\n",
            "90\n",
            "321\n",
            "--\n",
            "http://news.bbc.co.uk\n",
            "52\n",
            "260\n",
            "--\n",
            "http://www.usatoday.com\n",
            "25\n",
            "152\n",
            "--\n",
            "http://www.latimes.com\n",
            "0\n",
            "0\n",
            "--\n",
            "              name  ...                            url\n",
            "0       news.yahoo  ...          http://news.yahoo.com\n",
            "1      news.google  ...         http://news.google.com\n",
            "2   huffingtonpost  ...  http://www.huffingtonpost.com\n",
            "3              cnn  ...             http://www.cnn.com\n",
            "4          nytimes  ...         http://www.nytimes.com\n",
            "5          foxnews  ...         http://www.foxnews.com\n",
            "6          nbcnews  ...         http://www.nbcnews.com\n",
            "7        dailymail  ...     http://www.dailymail.co.uk\n",
            "8   washingtonpost  ...  http://www.washingtonpost.com\n",
            "9      theguardian  ...     http://www.theguardian.com\n",
            "10             wsj  ...             http://www.wsj.com\n",
            "11      abcnews.go  ...      http://www.abcnews.go.com\n",
            "12        news.bbc  ...          http://news.bbc.co.uk\n",
            "13        usatoday  ...        http://www.usatoday.com\n",
            "14         latimes  ...         http://www.latimes.com\n",
            "\n",
            "[15 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUTMorDBU8tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mounting google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh7nJCdTS5DC",
        "colab_type": "code",
        "outputId": "226be59f-851b-43b5-b9c2-deb9857d2ece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#save the output table\n",
        "\n",
        "timestr = time.strftime(\"%Y%m%d\")\n",
        "filename = 'heatmap' + timestr + '.csv'\n",
        "\n",
        "with open('heatmap' + timestr + '.csv', 'w', newline='') as csvfile:\n",
        "    heatmap = csv.writer(csvfile, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "#   heatmap.writerow(['name', 'appearances','size','url'])\n",
        "\n",
        "    for item in output_df:\n",
        "        heatmap.writerow(output_df[item])\n",
        "\n",
        "print(timestr)\n",
        "print(filename)\n",
        "\n",
        "#save output table as .csv to google drive\n",
        "\n",
        "!cp heatmap20200503.csv \"drive/My Drive/heatmap\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20200505\n",
            "heatmap20200505.csv\n",
            "cp: cannot stat 'heatmap20200503.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTivOWxklhaX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "6559378a-c3ed-4d75-e839-55643b115a1c"
      },
      "source": [
        "output_df.to_csv('heatmap' + timestr + '.csv', index = 0, delimiter=';')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-69ee672867d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'heatmap'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimestr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: to_csv() got an unexpected keyword argument 'delimiter'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rozGoBBrOj9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#investigate the input website\n",
        "\n",
        "url = 'http://www.ebizmba.com/articles/news-websites'\n",
        "site = request.urlopen(url)\n",
        "soupfile = BeautifulSoup(site)\n",
        "\n",
        "soupfile.prettify()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}