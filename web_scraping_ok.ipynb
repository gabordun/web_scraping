{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabordun/web_scraping/blob/master/web_scraping_ok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSINI_S8QRib",
        "colab_type": "text"
      },
      "source": [
        "**Web scraping project to create a COVID-19 heat map**\n",
        "\n",
        "The aim is to count the uniqe mentions of Coronavirus on the world's most popular websites.\n",
        "\n",
        "The list of the most popular websites comes from:\n",
        "http://www.ebizmba.com/articles/news-websites\n",
        "\n",
        "The script **scrapes this site for the URLs** of the mostly visited news websites worldwide.\n",
        "\n",
        "After getting the URLs, **the script creates a parsed object for every individual websites' startsite content**.\n",
        "\n",
        "Then, according to a pre-defined '**keywords**' list, the script counts the number of mentioning of all the keywords and put the numbers in an **output table**.\n",
        "\n",
        "The output table contains the name & and the exact URL of the corresponding website, the number of the keywords total appearances and a proxy for the size of the site (the last in order to get some comparable measure).\n",
        "\n",
        "Finally, in the last section the output table is saved on Google Drive. After that, the saved file is intended to import into Tableau to create a visualization of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivafqztuo-Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the URLs of the news sites\n",
        "\n",
        "#import packages\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "#import requests\n",
        "\n",
        "from urllib import request\n",
        "import urllib\n",
        "import requests\n",
        "\n",
        "#finding sources\n",
        "\n",
        "\n",
        "url = 'http://www.ebizmba.com/articles/news-websites'\n",
        "\n",
        "def getAllUrl(url):\n",
        "    try:\n",
        "        page = request.urlopen(url).read()\n",
        "    except:\n",
        "        return []\n",
        "    urlList = []\n",
        "    try:\n",
        "        soup = BeautifulSoup(page)\n",
        "        soup.prettify()\n",
        "        for anchor in soup.findAll('a', href=True):\n",
        "            if not 'http://' in anchor['href']:\n",
        "                if urlparse.urljoin(url, anchor['href']) not in urlList:\n",
        "                    urlList.append(urlparse.urljoin(url, anchor['href']))\n",
        "            else:\n",
        "                if anchor['href'] not in urlList:\n",
        "                    urlList.append(anchor['href'])\n",
        "\n",
        "        length = len(urlList)\n",
        "\n",
        "        return urlList\n",
        "    except request.HTTPError as e:\n",
        "        print(e)\n",
        "\n",
        "print(getAllUrl(url))\n",
        "\n",
        "result_url_list= getAllUrl(url)\n",
        "not_needed = ['ebizmba','alexa','quantcast','siteanalytics']\n",
        "\n",
        "#filter unnecessary items\n",
        "\n",
        "for item in getAllUrl(url):\n",
        " for i in not_needed:\n",
        "  if i in (item):\n",
        "   result_url_list.remove(item)\n",
        "\n",
        "print(result_url_list)\n",
        "print(len(getAllUrl(url)))\n",
        "len(result_url_list)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t60DWdLQpTfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get parsed objects from each URLs - optional\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for url in result_url_list:\n",
        "  try: \n",
        "   basic=requests.get(url).text\n",
        "   soup=BeautifulSoup(basic, 'html.parser')\n",
        "  except:\n",
        "   pass\n",
        "  \n",
        "  #stripped_text=soup.get_text()\n",
        "  corpus.append(soup)\n",
        "\n",
        "len(corpus) \n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svkmk540GGpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run keyword(s) counter, create output table\n",
        "\n",
        "table = {'name': [], 'number of mentioning':[] , 'site size': [],'url':[] }\n",
        "\n",
        "keywords=['corona','Corona','coronavirus','corona virus','Corona Virus', 'Coronavirus', 'COVID', 'COVID-19']\n",
        "\n",
        "for url in result_url_list:\n",
        "  try: \n",
        "   basic=requests.get(url).text\n",
        "   soup=BeautifulSoup(basic, 'html.parser')\n",
        "  except:\n",
        "   pass\n",
        "  \n",
        "  counter=0\n",
        "\n",
        "  for i in keywords:\n",
        "   y=len(soup.find_all(text=re.compile(i)))\n",
        "   counter += y\n",
        "  \n",
        "  size = len(soup.find_all('a'))\n",
        "\n",
        "  print(url)\n",
        "  print(counter)\n",
        "  print(size)\n",
        "  print('--')\n",
        "\n",
        "  table['name'].append(urllib.parse.urlsplit(url)[1].replace('www.','').replace('.com','').replace('.co.uk',''))\n",
        "  table['number of mentioning'].append(counter)\n",
        "  table['site size'].append(size)\n",
        "  table['url'].append(url)\n",
        "\n",
        "output_df = pd.DataFrame(data = table)\n",
        "print(output_df)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUTMorDBU8tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mounting google drive - need to be run only for the first time!\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh7nJCdTS5DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save the output table\n",
        "\n",
        "timestr = time.strftime(\"%Y%m%d\")\n",
        "\n",
        "with open('heatmap' + timestr + '.csv', 'w', newline='') as csvfile:\n",
        "    heatmap = csv.writer(csvfile, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "    for name in output_df:\n",
        "        heatmap.writerow(output_df[name])\n",
        "\n",
        "print(timestr)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rozGoBBrOj9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#investigate the input website\n",
        "\n",
        "url = 'http://www.ebizmba.com/articles/news-websites'\n",
        "site = request.urlopen(url)\n",
        "soupfile = BeautifulSoup(site)\n",
        "\n",
        "soupfile.prettify()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}